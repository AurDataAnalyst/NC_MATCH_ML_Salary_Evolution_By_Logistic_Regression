{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"http://www.indeed.fr/jobs?q=data+scientist&l=Paris&start=10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_job_title_from_result(soup): \n",
    "#     jobs = []\n",
    "#     for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "#         for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "#             jobs.append(a[\"title\"])\n",
    "#     return(jobs)\n",
    "# extract_job_title_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_company_from_result(soup): \n",
    "#     companies = []\n",
    "#     for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "#         company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
    "#         if len(company) > 0:\n",
    "#             for b in company:\n",
    "#                 companies.append(b.text.strip())\n",
    "#         else:\n",
    "#                 sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
    "#                 for span in sec_try:\n",
    "#                     companies.append(span.text.strip())\n",
    "#     return(companies)\n",
    "# extract_company_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_location_from_result(soup): \n",
    "#     locations = []\n",
    "#     spans = soup.findAll('span', attrs={'class': 'location'})\n",
    "#     for span in spans:\n",
    "#         locations.append(span.text)\n",
    "#     return(locations)\n",
    "# extract_location_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_salary_from_result(soup): \n",
    "#     salaries = []\n",
    "#     for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "#         try:\n",
    "#                 div_one=div.find('nobr').text\n",
    "#                 div_six=div_one.replace(u'\\xa0', '')\n",
    "#                 salaries.append(div_six)\n",
    "#         except:\n",
    "#             try:\n",
    "#                 div_two = div.find(name=\"div\", attrs={\"class\":\"sjcl\"})\n",
    "#                 div_three = div_two.find(\"div\")\n",
    "#                 div_four=div_three.text.strip()\n",
    "#                 div_five=div_four.replace(u'\\xa0', '')\n",
    "#                 salaries.append(div_five)\n",
    "#             except:\n",
    "#                 salaries.append(\"Nothing_found\")\n",
    "#     return(salaries)\n",
    "# extract_salary_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url):\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "    df = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\", \"Synopsis\"])\n",
    "    for each in soup.find_all(class_= \"result\" ):\n",
    "        try: \n",
    "            title = each.find(class_='jobtitle').text.replace('\\n', '')\n",
    "        except:\n",
    "            title = 'None'\n",
    "        try:\n",
    "            location = each.find('span', {'class':\"location\" }).text.replace('\\n', '')\n",
    "        except:\n",
    "            location = 'None'\n",
    "        try: \n",
    "            company = each.find(class_='company').text.replace('\\n', '')\n",
    "        except:\n",
    "            company = 'None'\n",
    "        try:\n",
    "            salary = each.find('span', {'class':'no-wrap'}).text.replace(u'\\xa0', u'').replace('\\n', '')\n",
    "        except:\n",
    "            salary = 'None'\n",
    "        synopsis = each.find('div', {'class':'summary'}).text.replace('\\n', '')\n",
    "        df = df.append({'Title':title, 'Location':location, 'Company':company, 'Salary':salary, 'Synopsis':synopsis}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Data Engineer H/F</td>\n",
       "      <td>None</td>\n",
       "      <td>Octopeek</td>\n",
       "      <td>4000 € par mois</td>\n",
       "      <td>Proposer des architectures data adaptées aux b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist Junior</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>IPANEMA CONSULTING</td>\n",
       "      <td>None</td>\n",
       "      <td>Dans le cadre de son développement d’activité,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Data scientist / Data analyst Junior / Freelance</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Eurostaff group SAS</td>\n",
       "      <td>400 € par jour</td>\n",
       "      <td>Doit maîtriser parfaitement les outils statist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Junior Data Scientist (F/H)</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Euler Hermes Group</td>\n",
       "      <td>None</td>\n",
       "      <td>Demonstrated skills at data cleansing, data qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Data Scientist F/H</td>\n",
       "      <td>Paris 13e (75)</td>\n",
       "      <td>NATIXIS</td>\n",
       "      <td>None</td>\n",
       "      <td>Au sein du Data Innovation Lab, parmi une équi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>DATA SCIENTIST H/F</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Ad Scientiam</td>\n",
       "      <td>None</td>\n",
       "      <td>En tant que Data Scientist vous serez rattaché...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Data SCIENTIST</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>KAISENS DATA</td>\n",
       "      <td>42 € - 46 € par heure</td>\n",
       "      <td>Nous recherchons un data scientist qui aura po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Stage - IT Data Scientist</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>SCOR</td>\n",
       "      <td>None</td>\n",
       "      <td>Vous êtes à l’aise avec les méthodologies stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Alternance - Data Scientist Junior (H/F)</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>DSOgroup</td>\n",
       "      <td>None</td>\n",
       "      <td>De traduire les problématiques métiers en prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Data Scientist H/F</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Equancy</td>\n",
       "      <td>None</td>\n",
       "      <td>Equancy recherche un data scientist passionné ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Explorateur Data Scientist – Santé</td>\n",
       "      <td>Paris (75)</td>\n",
       "      <td>Alcimed</td>\n",
       "      <td>None</td>\n",
       "      <td>Dès votre arrivée, vous travaillez au croiseme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Stagiaire Data Scientist H/F</td>\n",
       "      <td>None</td>\n",
       "      <td>Ensembl'</td>\n",
       "      <td>None</td>\n",
       "      <td>ENSEMBL’, 1er réseau social d’échanges et d’en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title        Location  \\\n",
       "0                                  Data Engineer H/F            None   \n",
       "1                              Data Scientist Junior      Paris (75)   \n",
       "2   Data scientist / Data analyst Junior / Freelance      Paris (75)   \n",
       "3                        Junior Data Scientist (F/H)      Paris (75)   \n",
       "4                                 Data Scientist F/H  Paris 13e (75)   \n",
       "5                                 DATA SCIENTIST H/F      Paris (75)   \n",
       "6                                     Data SCIENTIST      Paris (75)   \n",
       "7                          Stage - IT Data Scientist      Paris (75)   \n",
       "8           Alternance - Data Scientist Junior (H/F)      Paris (75)   \n",
       "9                                 Data Scientist H/F      Paris (75)   \n",
       "10                Explorateur Data Scientist – Santé      Paris (75)   \n",
       "11                      Stagiaire Data Scientist H/F            None   \n",
       "\n",
       "                Company                 Salary  \\\n",
       "0              Octopeek        4000 € par mois   \n",
       "1    IPANEMA CONSULTING                   None   \n",
       "2   Eurostaff group SAS         400 € par jour   \n",
       "3    Euler Hermes Group                   None   \n",
       "4               NATIXIS                   None   \n",
       "5          Ad Scientiam                   None   \n",
       "6          KAISENS DATA  42 € - 46 € par heure   \n",
       "7                  SCOR                   None   \n",
       "8              DSOgroup                   None   \n",
       "9               Equancy                   None   \n",
       "10              Alcimed                   None   \n",
       "11             Ensembl'                   None   \n",
       "\n",
       "                                             Synopsis  \n",
       "0   Proposer des architectures data adaptées aux b...  \n",
       "1   Dans le cadre de son développement d’activité,...  \n",
       "2   Doit maîtriser parfaitement les outils statist...  \n",
       "3   Demonstrated skills at data cleansing, data qu...  \n",
       "4   Au sein du Data Innovation Lab, parmi une équi...  \n",
       "5   En tant que Data Scientist vous serez rattaché...  \n",
       "6   Nous recherchons un data scientist qui aura po...  \n",
       "7   Vous êtes à l’aise avec les méthodologies stat...  \n",
       "8   De traduire les problématiques métiers en prob...  \n",
       "9   Equancy recherche un data scientist passionné ...  \n",
       "10  Dès votre arrivée, vous travaillez au croiseme...  \n",
       "11  ENSEMBL’, 1er réseau social d’échanges et d’en...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_CITY = 'Rueil-Malmaison'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1000 results. 75 of these aren't rubbish.\n",
      "You have 2000 results. 95 of these aren't rubbish.\n",
      "You have 3000 results. 95 of these aren't rubbish.\n",
      "You have 4000 results. 95 of these aren't rubbish.\n",
      "You have 5000 results. 95 of these aren't rubbish.\n",
      "You have 6000 results. 95 of these aren't rubbish.\n",
      "You have 7000 results. 95 of these aren't rubbish.\n",
      "You have 8000 results. 97 of these aren't rubbish.\n",
      "You have 9000 results. 97 of these aren't rubbish.\n",
      "You have 10000 results. 97 of these aren't rubbish.\n",
      "You have 11000 results. 97 of these aren't rubbish.\n",
      "You have 12000 results. 97 of these aren't rubbish.\n",
      "You have 13000 results. 97 of these aren't rubbish.\n",
      "You have 14000 results. 97 of these aren't rubbish.\n",
      "You have 15000 results. 97 of these aren't rubbish.\n",
      "You have 16000 results. 97 of these aren't rubbish.\n",
      "You have 17000 results. 97 of these aren't rubbish.\n",
      "You have 18000 results. 97 of these aren't rubbish.\n",
      "You have 19000 results. 97 of these aren't rubbish.\n",
      "You have 20000 results. 97 of these aren't rubbish.\n",
      "You have 21000 results. 97 of these aren't rubbish.\n",
      "You have 22000 results. 97 of these aren't rubbish.\n",
      "You have 23000 results. 97 of these aren't rubbish.\n",
      "You have 24000 results. 97 of these aren't rubbish.\n",
      "You have 25000 results. 97 of these aren't rubbish.\n",
      "You have 26000 results. 97 of these aren't rubbish.\n",
      "You have 27000 results. 97 of these aren't rubbish.\n",
      "You have 28000 results. 97 of these aren't rubbish.\n",
      "You have 29000 results. 97 of these aren't rubbish.\n"
     ]
    }
   ],
   "source": [
    "url_template = \"http://www.indeed.fr/jobs?q=data+scientist&l={}&start={}\"\n",
    "max_results_per_city = 5000 # Set this to a high-value (5000) to generate more results. \n",
    "# Crawling more results, will also take much longer. First test your code on a small number of results and then expand.\n",
    "i = 0\n",
    "results = []\n",
    "df_more = pd.DataFrame(columns=[\"Title\",\"Location\",\"Company\",\"Salary\", \"Synopsis\"])\n",
    "for city in set(['Paris', 'Suresnes', 'Courbevoie', 'Nanterre', 'Puteaux', \n",
    "    'Neuilly-Sur-Seine', 'Boulogne-Billancourt', 'Issy-Les-Moulineaux', 'Gennevilliers', \n",
    "    'Levallois-Perret', 'Colombes', 'Bois-Colombes', 'Antony', 'Le Plessis-Robinson', YOUR_CITY, \n",
    "    'Malakoff', 'Sèvres', 'Clichy', 'Montrouge', 'Bagneux']):\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "        # Grab the results from the request (as above)\n",
    "        url = url_template.format(city, start)\n",
    "        # Append to the full set of results\n",
    "        html = requests.get(url)\n",
    "        soup = BeautifulSoup(html.content, 'html.parser', from_encoding=\"utf-8\")\n",
    "        for each in soup.find_all(class_= \"result\" ):\n",
    "            try: \n",
    "                title = each.find(class_='jobtitle').text.replace('\\n', '')\n",
    "            except:\n",
    "                title = None\n",
    "            try:\n",
    "                location = each.find('span', {'class':\"location\" }).text.replace('\\n', '')\n",
    "            except:\n",
    "                location = None\n",
    "            try: \n",
    "                company = each.find(class_='company').text.replace('\\n', '')\n",
    "            except:\n",
    "                company = None\n",
    "            try:\n",
    "                salary = each.find('span', {'class':'no-wrap'}).text.replace('\\n','').replace(u'\\xa0', u'')\n",
    "            except:\n",
    "                salary = None\n",
    "            try:\n",
    "                synopsis = each.find('div', {'class':'summary'}).text.replace('\\n', '')\n",
    "            except:\n",
    "                synopsis = None\n",
    "            df_more = df_more.append({'Title':title, 'Location':location, 'Company':company, 'Salary':salary, 'Synopsis':synopsis}, ignore_index=True)\n",
    "            i += 1\n",
    "            if i % 1000 == 0:  # Ram helped me build this counter to see how many. You can visibly see Ram's vernacular in the print statements.\n",
    "                print('You have ' + str(i) + ' results. ' + str(df_more.dropna().drop_duplicates().shape[0]) + \" of these aren't rubbish.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more.to_csv('Long_Projet_Indeed_Non_Nettoye.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more = pd.read_csv('Long_Projet_Indeed_Non_Nettoye.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "#df_more['Location']=df_more['Location'].replace({')': ''}, regex=True)\n",
    "df_more.Location.replace(')','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_more.head())\n",
    "print (df_more.shape)\n",
    "print (df_more[df_more.Salary != 'None'].shape)\n",
    "df_more = df_more[df_more.Salary != 'None'].drop_duplicates().dropna()\n",
    "print (df_more.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more = df_more[df_more.Salary.str.contains(\"heure\") == False]\n",
    "df_more = df_more[df_more.Salary.str.contains(\"mois\") == False]\n",
    "print (df_more.shape)\n",
    "df_more.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_stripper(dataframe, column):\n",
    "    dataframe[str(column)] = dataframe[str(column)].replace({'\\€':''}, regex = True)\n",
    "    dataframe[str(column)].replace(regex=True,inplace=True,to_replace=r'\\D',value=r' ')\n",
    "    dataframe[str(column)] = dataframe[str(column)].str.replace(' ',',')\n",
    "    dataframe = dataframe.join(dataframe[str(column)].str.split(',,,', 1, expand=True).rename(columns={0:'Low', 1:'High'}))\n",
    "    dataframe['Low'] = dataframe['Low'].str.replace(',','')\n",
    "    dataframe['Low'] = dataframe['Low'].astype('float64')\n",
    "    dataframe.drop(str(column), axis=1, inplace=True)\n",
    "    dataframe['High'] = dataframe['High'].str.replace(',','')\n",
    "    dataframe['High'] = dataframe['High'].apply(pd.to_numeric)\n",
    "    dataframe['Average'] = dataframe[['Low', 'High']].mean(axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more = salary_stripper(df_more, 'Salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more = df_more.join(df_more['Location'].str.split('(', 1, expand=True).rename(columns={0:'City', 1:'Department'}))\n",
    "#df_more=df_more.replace(')','')\n",
    "#df_more['Department']=df_more['Department'].replace({')': ''}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strip_state(x):\n",
    "#     if x != None:\n",
    "#         return x[0:3]\n",
    "#     else:\n",
    "#         None\n",
    "# df_more['State Initials'] = df_more['State'].apply(strip_state)\n",
    "# df_more.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more.to_csv('Long_Projet_Indeed_Nettoye.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more = pd.read_csv('Long_Projet_Indeed_Nettoye.csv', index_col=0)\n",
    "df_more = df_more.reset_index(drop=True)\n",
    "df_more.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_more['Department']=df_more['Department'].replace({')': ''}, regex=True)\n",
    "df_more = df_more.join(df_more['Department'].str.split(')', 8, expand=True).rename(columns={2:'Department', 3:'Nada'}))\n",
    "df_more.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more.rename(columns={\"0\": \"Departement\", \"1\": \"Vide\"})\n",
    "df_more.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = df_more['Average'].median()\n",
    "print ('The median salary for our data set is €' + str(median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def above_median(x):\n",
    "    if x > median:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Above Median'] = df_more['Average'].apply(above_median)\n",
    "df_more.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more['Above Median'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "# from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_dummy = pd.get_dummies(df_more['City'])\n",
    "df_city_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state_dummy = pd.get_dummies(df_more['Department'])\n",
    "df_state_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "X = df_city_dummy\n",
    "y = df_more['Above Median']\n",
    "cv_model = cross_val_score(model, X, y, cv=6)\n",
    "print ('Cross-validated scores:', cv_model)\n",
    "print ('Average score:', cv_model.mean())\n",
    "print ('Standard deviation of score:', cv_model.std())\n",
    "model.fit(X, y)\n",
    "print (model.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senior(x):\n",
    "    if 'Senior' in x:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Senior'] = df_more['Title'].apply(senior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more[df_more.Senior != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manager(x):\n",
    "    if 'Manager' in x:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Manager'] = df_more['Title'].apply(manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer(x):\n",
    "    if 'Engineer' in x:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Engineer'] = df_more['Title'].apply(engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more[df_more.Engineer != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scientist(x):\n",
    "    if 'Data Scientist' in x:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Data Scientist'] = df_more['Title'].apply(data_scientist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyst(x):\n",
    "    if 'Analyst' in x:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Analyst'] = df_more['Title'].apply(analyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assistant(x):\n",
    "    if 'Assistant' in x:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Assistant'] = df_more['Title'].apply(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage(x):\n",
    "    if 'stagiaire' in x:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Intern'] = df_more['Title'].apply(stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def developpeur(x):\n",
    "    if 'Developpeur' in x:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Developpeur'] = df_more['Title'].apply(developpeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def machine_learning(x):\n",
    "    if 'Machine Learning' in x:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_more['Machine Learning'] = df_more['Title'].apply(machine_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = df_more.copy(deep=True)\n",
    "feature_matrix.drop(['Title', 'Location', 'Company', 'Synopsis', 'Low', 'High', 'Average', 'City', 'Department', 'Above Median'], axis=1, inplace=True)\n",
    "print (feature_matrix.shape)\n",
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "X_features = pd.concat([df_city_dummy, feature_matrix, df_state_dummy], axis=1)\n",
    "y = df_more['Above Median']\n",
    "cv_model = cross_val_score(model, X_features, y, cv=6)\n",
    "print ('Cross-validated scores:', cv_model)\n",
    "print ('Average score:', cv_model.mean())\n",
    "print ('Standard deviation of score:', cv_model.std())\n",
    "model.fit(X_features, y)\n",
    "print (model.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvec = CountVectorizer(stop_words='english', max_features=30, ngram_range=(2,2))\n",
    "vectorizers = cvec.fit_transform(df_more['Title'].values)\n",
    "\n",
    "df_vec  = pd.DataFrame(vectorizers.todense(), columns=cvec.get_feature_names())\n",
    "print (df_vec.shape)\n",
    "df_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvec = pd.concat([df_vec, df_city_dummy, df_state_dummy], axis=1)\n",
    "X_cvec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "y = df_more['Above Median']\n",
    "cv_model = cross_val_score(model, X_cvec, y, cv=6)\n",
    "print ('Cross-validated scores:', cv_model)\n",
    "print ('Average score:', cv_model.mean())\n",
    "print ('Standard deviation of score:'), cv_model.std()\n",
    "model.fit(X_cvec, y)\n",
    "print (model.oob_score_)\n",
    "importance_dataframe = pd.DataFrame(model.feature_importances_, index = X_cvec.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "importance_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "scores_log = cross_val_score(log_reg, X_cvec, y, cv=6)\n",
    "print ('Cross-validated scores:', scores_log)\n",
    "print ('Average score:', scores_log.mean())\n",
    "print ('Standard deviation of score:', scores_log.std())\n",
    "log_model = log_reg.fit(X_cvec, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svmrbf = SVC(kernel='rbf')\n",
    "scores_svm = cross_val_score(model_svmrbf, X_cvec, y, cv=6)\n",
    "print ('Cross-validated scores:', scores_svm)\n",
    "print ('Average score:', scores_svm.mean())\n",
    "print ('Standard deviation of score:', scores_svm.std())\n",
    "svm_model = model_svmrbf.fit(X_cvec, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svmlm = SVC(kernel='linear')\n",
    "scores_svmlm = cross_val_score(model_svmlm, X_cvec, y, cv=6)\n",
    "print ('Cross-validated scores:', scores_svmlm)\n",
    "print ('Average score:', scores_svmlm.mean())\n",
    "print ('Standard deviation of score:', scores_svmlm.std())\n",
    "svm_model = model_svmlm.fit(X_cvec, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1, activation='relu')\n",
    "cv_model = cross_val_score(clf, X_cvec, y, cv=5)\n",
    "print ('Cross-validated scores:', cv_model)\n",
    "print ('Average score:', cv_model.mean())\n",
    "print ('Standard deviation of score:', cv_model.std())   \n",
    "clf.fit(feature_matrix, y)\n",
    "#clf.score(feature_matrix, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(100,), random_state=1, activation='relu')\n",
    "cv_model = cross_val_score(clf, X_cvec, y, cv=5)\n",
    "print ('Cross-validated scores:', cv_model)\n",
    "print ('Average score:', cv_model.mean())\n",
    "print ('Standard deviation of score:', cv_model.std())   \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in range(5):\n",
    "    y[:,i] = le.fit_transform(y[:,i])\n",
    "clf.fit(feature_matrix, y)\n",
    "#clf.score(feature_matrix, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_cvec, y)\n",
    "cv_model = cross_val_score(clf, X_cvec, y, cv=6)\n",
    "print ('Cross-validated scores:', cv_model)\n",
    "print ('Average score:', cv_model.mean())\n",
    "print ('Standard deviation of score:', cv_model.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier(booster='gbtree')\n",
    "cv_model = cross_val_score(model, X_cvec, y, cv=6)\n",
    "print ('Cross-validated scores:', cv_model)\n",
    "print ('Average score:', cv_model.mean())\n",
    "print ('Standard deviation of score:', cv_model.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_synos = CountVectorizer(stop_words='english', max_features=50)\n",
    "vectorizers_synos = cvec_synos.fit_transform(df_more['Synopsis']).toarray()\n",
    "df_vec_synos  = pd.DataFrame(vectorizers_synos, columns=cvec_synos.get_feature_names())\n",
    "print (df_vec_synos.shape)\n",
    "df_vec_synos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvec_synos = pd.concat([df_vec, df_city_dummy, df_state_dummy, df_vec_synos], axis=1)\n",
    "X_cvec_synos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvec_synos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, oob_score=True)\n",
    "y = df_more['Above Median']\n",
    "model.fit(X_cvec_synos, y)\n",
    "print (model.oob_score_)\n",
    "cv_model = cross_val_score(model, X_cvec_synos, y, cv=6)\n",
    "print ('Cross-validated scores:', cv_model)\n",
    "print ('Average score:', cv_model.mean())\n",
    "print ('Standard deviation of score:', cv_model.std())\n",
    "importance_dataframe_big = pd.DataFrame(model.feature_importances_, index = X_cvec_synos.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "importance_dataframe_big.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "log_reg = linear_model.LogisticRegression()\n",
    "log_model = log_reg.fit(X_cvec_synos, y)\n",
    "scores_log = cross_val_score(log_reg, X_cvec_synos, y, cv=6)\n",
    "print ('Cross-validated scores:', scores_log)\n",
    "print ('Average score:', scores_log.mean())\n",
    "print ('Standard deviation of score:', scores_log.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svmrbf = SVC(kernel='rbf')\n",
    "scores_svm = cross_val_score(model_svmrbf, X_cvec_synos, y, cv=6)\n",
    "print ('Cross-validated scores:', scores_svm)\n",
    "print ('Average score:', scores_svm.mean())\n",
    "print ('Standard deviation of score:', scores_svm.std())\n",
    "svm_model = model_svmrbf.fit(X_cvec_synos, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = linear_model.LogisticRegression()\n",
    "log_model = log_reg.fit(df_vec, y)\n",
    "scores_log = cross_val_score(log_reg, df_vec, y, cv=6)\n",
    "print ('Cross-validated scores:', scores_log)\n",
    "print ('Average score:', scores_log.mean())\n",
    "print ('Standard deviation of score:', scores_log.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = zip(cvec.get_feature_names(), log_model.coef_[0])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = linear_model.LogisticRegression()\n",
    "log_model = log_reg.fit(df_state_dummy, y)\n",
    "scores_log = cross_val_score(log_reg, df_state_dummy, y, cv=6)\n",
    "print ('Cross-validated scores:', scores_log)\n",
    "print ('Average score:', scores_log.mean())\n",
    "print ('Standard deviation of score:', scores_log.std())\n",
    "b = zip(df_state_dummy.columns, log_model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
